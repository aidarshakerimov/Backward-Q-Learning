{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled17.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMIJzoixZxy/nk4x2WMHO7n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aidarshakerimov/Backward-Q-Learning/blob/master/Untitled17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDWHpQQDdQRs",
        "outputId": "95f09233-9441-4edb-868c-f65d4174d283",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random\n",
        "\n",
        "    \n",
        "# Creates a table of Q_values (state-action) initialized with zeros\n",
        "# Initialize Q(s, a), for all s ∈ S, a ∈ A(s), arbitrarily, and Q(terminal-state, ·) = 0.\n",
        "def createQ_table(rows = 5, cols = 12):\n",
        "    \"\"\"\n",
        "    Implementation of creating a table for the Q(s, a) 'value state-action pair'\n",
        "    \n",
        "    Args:\n",
        "        rows -- type(int) Number of rows the simple grid world\n",
        "        cols -- type(int) Number of columns in the simple grid world\n",
        "    \n",
        "    Returns:\n",
        "        q_table -- type(np.array) 2D representation of state-action pair table \n",
        "                                     Rows are actions and columns are the states. \n",
        "    \"\"\"\n",
        "\n",
        "#                                 should be 5x12\n",
        "\n",
        "    # initialize the q_table with all zeros for each state and action\n",
        "    q_table = np.zeros((4, cols * rows))\n",
        "\n",
        "    # define an action dictionary to access corresponding state-action pairs fast\n",
        "    action_dict =  {\"NORTH\": q_table[0, :],\"WEST\": q_table[1, :], \"EAST\": q_table[2, :], \"SOUTH\": q_table[3, :]}\n",
        "    \n",
        "    return q_table\n",
        "\n",
        "\n",
        "# Choosing action using policy\n",
        "# Sutton's code pseudocode: Choose A from S using policy derived from Q (e.g., ε-greedy)\n",
        "# %10 exploration to avoid stucking at a local optima\n",
        "\n",
        "\n",
        "###### CHANGE TO BOLTZMAN\n",
        "def boltz_policy(state, q_table, temp):\n",
        "    \"\"\"\n",
        "    Boltzmann distribution policy determines probability distribution of actions according\n",
        "    to Boltzmann distribution.\n",
        "    P(a_i) = exp(Q(s, a_i)/temp) / (sum of exp(Q/temp) over all actions)\n",
        "    \"\"\"\n",
        "#                                   sure 4????\n",
        "    # Store probabilities of each action in an array\n",
        "    pr_a = np.zeros(4, dtype=float) \n",
        "    for i in range(4):\n",
        "        pr_a[i] = math.exp(q_table[i, state]/temp)\n",
        "    sum_pr = np.sum(pr_a)\n",
        "    pr_a = pr_a / sum_pr\n",
        "\n",
        "    choose_action = np.random.random()\n",
        "    prob = 0\n",
        "    for i in range(4):\n",
        "        prob += pr_a[i]\n",
        "        if choose_action < prob :\n",
        "            return i\n",
        "\n",
        "def epsilon_greedy_policy(state, q_table, epsilon = 0.1):\n",
        "    \"\"\"\n",
        "    Epsilon greedy policy implementation takes the current state and q_value table \n",
        "    Determines which action to take based on the epsilon-greedy policy\n",
        "    \n",
        "    Args:\n",
        "        epsilon -- type(float) Determines exploration/explotion ratio\n",
        "        state -- type(int) Current state of the agent value between [0:47]\n",
        "        q_table -- type(np.array) Determines state value\n",
        "        \n",
        "    Returns:\n",
        "        action -- type(int) Choosen function based on Q(s, a) pairs & epsilon\n",
        "    \"\"\"\n",
        "    # choose a random int from an uniform distribution [0.0, 1.0) \n",
        "    decide_explore_exploit  = np.random.random()\n",
        "    \n",
        "    if(decide_explore_exploit < epsilon):\n",
        "        action = np.random.choice(4) # UP = 0, LEFT = 1, RIGHT = 2, DOWN = 3\n",
        "    else:\n",
        "        action = np.argmax(q_table[:, state]) # Choose the action with largest Q-value (state value)\n",
        "        \n",
        "    return action\n",
        "    \n",
        "\n",
        "\n",
        "def move_agent(agent, action):\n",
        "    \"\"\"\n",
        "    Moves the agent based on action to take\n",
        "    \n",
        "    Args:\n",
        "        agent -- type(tuple) x, y coordinate of the agent on the grid\n",
        "        action -- type(int) updates agent's position \n",
        "        \n",
        "    Returns:\n",
        "        agent -- type(tuple) new coordinate of the agent\n",
        "    \"\"\"\n",
        "    # get position of the agent\n",
        "    (posX , posY) = agent\n",
        "    # UP \n",
        "    if ((action == 0) and posX > 0):\n",
        "        posX = posX - 1\n",
        "    # LEFT\n",
        "    if((action == 1) and (posY > 0)):\n",
        "        posY = posY - 1\n",
        "    # RIGHT\n",
        "    if((action == 2) and (posY < 11)):\n",
        "        posY = posY + 1\n",
        "    # DOWN                                     <4!\n",
        "    if((action) == 3 and (posX < 4)):\n",
        "        posX = posX + 1\n",
        "    agent = (posX, posY)\n",
        "    \n",
        "    return agent\n",
        "\n",
        "\n",
        "def get_state(agent, q_table):\n",
        "    \"\"\"\n",
        "    Determine the state and state value given agent's position\n",
        "    \n",
        "    Args:\n",
        "        agent -- type(tuple) x, y coordinate of the agent on the grid\n",
        "        q_table -- type(np.array) Determines state value\n",
        "        \n",
        "    Returns:\n",
        "        state -- type(int) state value between \n",
        "        max_state_value -- type(float) maximum state value at the position of the agent\n",
        "    \"\"\"\n",
        "    # get position of the agent\n",
        "    (posX , posY) = agent\n",
        "    \n",
        "    # obtain the state value\n",
        "    state = 12 * posX + posY\n",
        "### THIS IS OK SINCE THEY CAN BE NUMERATED EASILY\n",
        "\n",
        "    # get maximum state value from the table\n",
        "    state_action = q_table[:, int(state)]\n",
        "    maximum_state_value = np.amax(state_action) # return the state value with for the highest action\n",
        "    return state, maximum_state_value\n",
        "\n",
        "def get_reward(state):\n",
        "    \"\"\"\n",
        "    Function returns reward in the given state\n",
        "    \n",
        "    Args:\n",
        "        state -- type(int) state value between [0,47]\n",
        "        \n",
        "    Returns: \n",
        "        reward -- type(int) Reward in the corresponding state \n",
        "        game_end -- type(bool) Flag indicates game end (falling out of cliff / reaching the goal)\n",
        "    \"\"\"\n",
        "    # game continues\n",
        "    game_end = False\n",
        "    # all states except cliff have -1 value\n",
        "    reward = -1\n",
        "    # goal state                        NEED TO BE 59\n",
        "    if(state == 59):\n",
        "        game_end = True\n",
        "        reward = 10\n",
        "    # cliff                             NEED TO BE 49-58\n",
        "    if(state >= 49 and state <= 58):\n",
        "        game_end = True\n",
        "        # Penalize the agent if agent encounters a cliff CHANGED FROM -100 TO -10\n",
        "        reward = -10\n",
        "\n",
        "    return reward, game_end\n",
        "\n",
        "def update_qTable(q_table, state, action, reward, next_state_value, gamma_discount = 0.95, alpha = 0.9):\n",
        "    \"\"\"\n",
        "    Update the q_table based on observed rewards and maximum next state value\n",
        "    Sutton's Book pseudocode:  Q(S, A) <- Q(S, A) + [alpha * (reward + (gamma * maxValue(Q(S', A'))) -  Q(S, A) ]\n",
        "    \n",
        "    Args:\n",
        "        q_table -- type(np.array) Determines state value\n",
        "        state -- type(int) state value between [0,47]\n",
        "        action -- type(int) action value [0:3] -> [UP, LEFT, RIGHT, DOWN]\n",
        "        reward -- type(int) reward in the corresponding state \n",
        "        next_state_value -- type(float) maximum state value at next state\n",
        "        gamma_discount -- type(float) discount factor determines importance of future rewards\n",
        "        alpha -- type(float) controls learning convergence\n",
        "        \n",
        "    Returns:\n",
        "        q_table -- type(np.array) Determines state value\n",
        "    \"\"\"\n",
        "\n",
        "########                           ADD REFERENCE FORMULA\n",
        "\n",
        "    update_q_value = q_table[action, state] + alpha * (reward + (gamma_discount * next_state_value) - q_table[action, state])\n",
        "    q_table[action, state] = update_q_value\n",
        "\n",
        "    return q_table    \n",
        "\n",
        "def qlearning(num_episodes = 500, gamma_discount = 0.9, alphax = 0.9, epsilon = 0.1):\n",
        "    \"\"\"\n",
        "    Implementation of q-learning algorithm. (Sutton's book)\n",
        "    \n",
        "    Args:\n",
        "        num_episodes -- type(int) number of games to train agent\n",
        "        gamma_discount -- type(float) discount factor determines importance of future rewards\n",
        "        alpha -- type(float) determines convergence rate of the algorithm (can think as updating states fast or slow)\n",
        "        epsilon -- type(float) explore/ exploit ratio (exe: default value 0.1 indicates %10 exploration)\n",
        "        \n",
        "    Returns:\n",
        "        q_table -- type(np.array) Determines state value\n",
        "        reward_cache -- type(list) contains cumulative_reward\n",
        "    \"\"\"\n",
        "    # initialize all states to 0\n",
        "    # Terminal state cliff_walking ends\n",
        "    reward_cache = list()\n",
        "    step_cache = list()\n",
        "    q_table = createQ_table()\n",
        "    a = random.randint(0,3)\n",
        "    b = random.randint(0,11)\n",
        "    agent = (a, b) # starting from left down corner\n",
        "    # start iterating through the episodes\n",
        "    temper = 8.79\n",
        "    alpha = alphax\n",
        "    gcount = 0\n",
        "    for episode in range(0, num_episodes):\n",
        "#                            ALSO CHANGE FOR 5\n",
        "        env = np.zeros((5, 12))\n",
        "        env = visited_env(agent, env)\n",
        "        agent = (a, b) \n",
        "# get the state from agent's position\n",
        "        state, _ = get_state(agent, q_table)\n",
        "        game_end = False\n",
        "        reward_cum = 0 # cumulative reward of the episode\n",
        "        step_cum = 0 # keeps number of iterations untill the end of the game\n",
        "        while(game_end == False):\n",
        "            # choose action using epsilon-greedy policy\n",
        "            action = boltz_policy(state, q_table, temper)\n",
        "            # move agent to the next state\n",
        "            agent = move_agent(agent, action)\n",
        "            step_cum += 1\n",
        "            env = visited_env(agent, env) # mark the visited path\n",
        "            # observe next state value\n",
        "            next_state, max_next_state_value = get_state(agent, q_table)\n",
        "            # observe reward and determine whether game ends\n",
        "            reward, game_end = get_reward(next_state)\n",
        "            reward_cum += reward \n",
        "            #prepare for tests\n",
        "            if next_state == 59:\n",
        "                gcount= gcount + 1\n",
        "            # update q_table\n",
        "            q_table = update_qTable(q_table, state, action, reward, max_next_state_value, gamma_discount, alpha)\n",
        "            # update the state\n",
        "            state = next_state\n",
        "        reward_cache.append(reward_cum)\n",
        "        if(gcount == 300):\n",
        "            print(\"Agent trained with Q-learning after 300 goal reachings\")\n",
        "            print(env) # display the last 2 path agent takes \n",
        "            print(\"cumulative\")\n",
        "            print(sum(reward_cache) / len(reward_cache))\n",
        "            print(\"Length of episode\")\n",
        "            print(sum(step_cache) / len(step_cache))\n",
        "            x = 0\n",
        "            y = 0\n",
        "            totalsteps = 0\n",
        "            goals = 0\n",
        "            for episode in range(0, 48):\n",
        "                newgcount = 0\n",
        "                env = np.zeros((5, 12))\n",
        "                env = visited_env(agent, env)\n",
        "                agent = (y, x) # starting from left down corner\n",
        " # get the state from agent's position\n",
        "                state, _ = get_state(agent, q_table)\n",
        "                game_end = False\n",
        "                if x < 11:\n",
        "                    if y == 3:\n",
        "                        y = 0\n",
        "                        x=x+1\n",
        "                    else:\n",
        "                        y = y + 1\n",
        "                else: \n",
        "                    if y == 3:\n",
        "                        y = 0\n",
        "                        x = 0\n",
        "                    else: \n",
        "                        y = y + 1\n",
        "                reward_cum = 0 # cumulative reward of the episode\n",
        "                step_cum = 0 # keeps number of iterations untill the end of the game\n",
        "                trials = 0\n",
        "                while(game_end == False):\n",
        "                   \n",
        "                    # choose action using epsilon-greedy policy\n",
        "                    action = boltz_policy(state, q_table, temper)\n",
        "                    # move agent to the next state\n",
        "                    agent = move_agent(agent, action)\n",
        "                    step_cum += 1\n",
        "                    env = visited_env(agent, env) # mark the visited path\n",
        "                    # observe next state value\n",
        "                    next_state, max_next_state_value = get_state(agent, q_table)\n",
        "                    # observe reward and determine whether game ends\n",
        "                    reward, game_end = get_reward(next_state)\n",
        "                    reward_cum += reward\n",
        "                    # update q_table\n",
        "                    q_table = update_qTable(q_table, state, action, reward, max_next_state_value, gamma_discount, alpha)\n",
        "                    # update the state\n",
        "                    state = next_state\n",
        "                    if state == 59:\n",
        "                        goals = goals + 1\n",
        "                    trials = trials + 1\n",
        "                    if trials == 40:\n",
        "                        game_end = True\n",
        "                reward_cache.append(reward_cum)\n",
        "                totalsteps = totalsteps +  step_cum\n",
        "                step_cache.append(step_cum)\n",
        "                temper = 0.99*temper\n",
        "            print(\"TS\")\n",
        "            print(totalsteps)\n",
        "            if (goals == 48):\n",
        "                print(\"success\")\n",
        "            else: \n",
        "                print(\"not success\")\n",
        "                print(goals/48)\n",
        "            return q_table, reward_cache, step_cache\n",
        "        step_cache.append(step_cum)\n",
        "        temper = 0.99*temper\n",
        "    return q_table, reward_cache, step_cache\n",
        "\n",
        "def sarsa(num_episodes = 500, gamma_discount = 0.95, alpha = 0.5, epsilon = 0.1):\n",
        "    \"\"\"\n",
        "    Implementation of SARSA algorithm. (Sutton's book)\n",
        "    \n",
        "    Args:\n",
        "        num_episodes -- type(int) number of games to train agent\n",
        "        gamma_discount -- type(float) discount factor determines importance of future rewards\n",
        "        alpha -- type(float) determines convergence rate of the algorithm (can think as updating states fast or slow)\n",
        "        epsilon -- type(float) explore/ exploit ratio (exe: default value 0.1 indicates %10 exploration)\n",
        "        \n",
        "    Returns:\n",
        "        q_table -- type(np.array) Determines state value\n",
        "        reward_cache -- type(list) contains cumulative_reward\n",
        "    \"\"\"\n",
        "    # initialize all states to 0\n",
        "    # Terminal state cliff_walking ends\n",
        "    q_table = createQ_table()\n",
        "    a = random.randint(0,3)\n",
        "    b = random.randint(0,11)\n",
        "    agent = (a, b) \n",
        "    step_cache = list()\n",
        "    reward_cache = list()\n",
        "    # start iterating through the episodes\n",
        "    temper = 8.79\n",
        "    for episode in range(0, num_episodes):\n",
        "        agent = (a, b) \n",
        "        game_end = False\n",
        "        reward_cum = 0 # cumulative reward of the episode\n",
        "        step_cum = 0 # keeps number of iterations untill the end of the game\n",
        "        env = np.zeros((5, 12))\n",
        "        env = visited_env(agent, env)\n",
        "        # choose action using policy\n",
        "        state, _ = get_state(agent, q_table)\n",
        "        action = boltz_policy(state, q_table, temper)\n",
        "        while(game_end == False):\n",
        "            # move agent to the next state\n",
        "            agent = move_agent(agent, action)\n",
        "            env = visited_env(agent, env)\n",
        "            step_cum += 1\n",
        "            # observe next state value\n",
        "            next_state, _ = get_state(agent, q_table)\n",
        "            # observe reward and determine whether game ends\n",
        "            reward, game_end = get_reward(next_state)\n",
        "            reward_cum += reward \n",
        "            # choose next_action using policy and next state\n",
        "            next_action = boltz_policy(next_state, q_table, temper)\n",
        "            # update q_table\n",
        "            next_state_value = q_table[next_action][next_state] # differs from q-learning uses the next action determined by policy\n",
        "            q_table = update_qTable(q_table, state, action, reward, next_state_value, gamma_discount, alpha)\n",
        "            # update the state and action\n",
        "            state = next_state\n",
        "            action = next_action # differs q_learning both state and action must updated\n",
        "        reward_cache.append(reward_cum)\n",
        "        step_cache.append(step_cum)\n",
        "        if(episode > 498):\n",
        "            print(\"Agent trained with SARSA after 500 iterations\")\n",
        "            print(env) # display the last 2 path agent takes \n",
        "        temper = 0.99*temper\n",
        "    return q_table, reward_cache, step_cache\n",
        "\n",
        "def visited_env(agent, env):\n",
        "    \"\"\"\n",
        "        Visualize the path agent takes\n",
        "        \n",
        "    \"\"\"\n",
        "    (posY, posX) = agent\n",
        "    env[posY][posX] = 1\n",
        "    return env\n",
        "    \n",
        "    \n",
        "def retrieve_environment(q_table, action):\n",
        "    \"\"\"\n",
        "    Displays the environment state values for a specific action\n",
        "    Implemented for debug purposes\n",
        "    \n",
        "    Args:\n",
        "        q_table -- type(np.array) Determines state value\n",
        "        action -- type(int) action value [0:3] -> [UP, LEFT, RIGHT, DOWN]\n",
        "    \"\"\"\n",
        "    env = q_table[action, :].reshape((5, 12))\n",
        "    print(env) # display environment values\n",
        "    \n",
        "def plot_cumreward_normalized(reward_cache_qlearning, reward_cache_SARSA):\n",
        "    \"\"\"\n",
        "    Visualizes the reward convergence\n",
        "    \n",
        "    Args:\n",
        "        reward_cache -- type(list) contains cumulative_reward\n",
        "    \"\"\"\n",
        "    cum_rewards_q = []\n",
        "    rewards_mean = np.array(reward_cache_qlearning).mean()\n",
        "    rewards_std = np.array(reward_cache_qlearning).std()\n",
        "    count = 0 # used to determine the batches\n",
        "    cur_reward = 0 # accumulate reward for the batch\n",
        "    for cache in reward_cache_qlearning:\n",
        "        count = count + 1\n",
        "        cur_reward += cache\n",
        "        if(count == 10):\n",
        "            # normalize the sample\n",
        "            normalized_reward = (cur_reward - rewards_mean)/rewards_std\n",
        "            cum_rewards_q.append(normalized_reward)\n",
        "            cur_reward = 0\n",
        "            count = 0\n",
        "            \n",
        "    cum_rewards_SARSA = []\n",
        "    rewards_mean = np.array(reward_cache_SARSA).mean()\n",
        "    rewards_std = np.array(reward_cache_SARSA).std()\n",
        "    count = 0 # used to determine the batches\n",
        "    cur_reward = 0 # accumulate reward for the batch\n",
        "    for cache in reward_cache_SARSA:\n",
        "        count = count + 1\n",
        "        cur_reward += cache\n",
        "        if(count == 10):\n",
        "            # normalize the sample\n",
        "            normalized_reward = (cur_reward - rewards_mean)/rewards_std\n",
        "            cum_rewards_SARSA.append(normalized_reward)\n",
        "            cur_reward = 0\n",
        "            count = 0      \n",
        "    # prepare the graph    \n",
        "    plt.plot(cum_rewards_q, label = \"q_learning\")\n",
        "    plt.plot(cum_rewards_SARSA, label = \"SARSA\")\n",
        "    plt.ylabel('Cumulative Rewards')\n",
        "    plt.xlabel('Batches of Episodes (sample size 10) ')\n",
        "    plt.title(\"Q-Learning/SARSA Convergence of Cumulative Reward\")\n",
        "    plt.legend(loc='lower right', ncol=2, mode=\"expand\", borderaxespad=0.)\n",
        "    plt.savefig('as.png')\n",
        "    \n",
        "def plot_number_steps(step_cache_qlearning, step_cache_SARSA):\n",
        "    \"\"\"\n",
        "        Visualize number of steps taken\n",
        "    \"\"\"    \n",
        "    cum_step_q = []\n",
        "    steps_mean = np.array(step_cache_qlearning).mean()\n",
        "    steps_std = np.array(step_cache_qlearning).std()\n",
        "    count = 0 # used to determine the batches\n",
        "    cur_step = 0 # accumulate reward for the batch\n",
        "    for cache in step_cache_qlearning:\n",
        "        count = count + 1\n",
        "        cur_step += cache\n",
        "        if(count == 10):\n",
        "            # normalize the sample\n",
        "            normalized_step = (cur_step - steps_mean)/steps_std\n",
        "            cum_step_q.append(normalized_step)\n",
        "            cur_step = 0\n",
        "            count = 0\n",
        "            \n",
        "    cum_step_SARSA = []\n",
        "    steps_mean = np.array(step_cache_SARSA).mean()\n",
        "    steps_std = np.array(step_cache_SARSA).std()\n",
        "    count = 0 # used to determine the batches\n",
        "    cur_step = 0 # accumulate reward for the batch\n",
        "    for cache in step_cache_SARSA:\n",
        "        count = count + 1\n",
        "        cur_step += cache\n",
        "        if(count == 10):\n",
        "            # normalize the sample\n",
        "            normalized_step = (cur_step - steps_mean)/steps_std\n",
        "            cum_step_SARSA.append(normalized_step)\n",
        "            cur_step = 0\n",
        "            count = 0      \n",
        "    # prepare the graph    \n",
        "    plt.plot(cum_step_q, label = \"q_learning\")\n",
        "    plt.plot(cum_step_SARSA, label = \"SARSA\")\n",
        "    plt.ylabel('Number of iterations')\n",
        "    plt.xlabel('Batches of Episodes (sample size 10) ')\n",
        "    plt.title(\"Q-Learning/SARSA Iteration number untill game ends\")\n",
        "    plt.legend(loc='lower right', ncol=2, mode=\"expand\", borderaxespad=0.)\n",
        "    plt.savefig('sa.png')\n",
        "    \n",
        "\n",
        "    \n",
        "def plot_qlearning_smooth(reward_cache):\n",
        "    \"\"\"\n",
        "    Visualizes the reward convergence using weighted average of previous 10 cumulative rewards\n",
        "    NOTE: Normalization gives better visualization\n",
        "    \n",
        "    Args:\n",
        "        reward_cache -- type(list) contains cumulative_rewards for episodes\n",
        "    \"\"\"\n",
        "    mean_rev = (np.array(reward_cache[0:11]).sum())/10\n",
        "    # initialize with cache mean\n",
        "    cum_rewards = [mean_rev] * 10\n",
        "    idx = 0\n",
        "    for cache in reward_cache:\n",
        "        cum_rewards[idx] = cache\n",
        "        idx += 1\n",
        "        smooth_reward = (np.array(cum_rewards).mean())\n",
        "        cum_rewards.append(smooth_reward)\n",
        "        if(idx == 10):\n",
        "            idx = 0\n",
        "        \n",
        "    plt.plot(cum_rewards)\n",
        "    plt.ylabel('Cumulative Rewards')\n",
        "    plt.xlabel('Batches of Episodes (sample size 10) ')\n",
        "    plt.title(\"Q-Learning  Convergence of Cumulative Reward\")\n",
        "    plt.legend(loc='lower left', ncol=2, mode=\"expand\", borderaxespad=0.)\n",
        "    plt.savefig('sa.png')\n",
        "\n",
        "def generate_heatmap(q_table):\n",
        "    \"\"\"\n",
        "        Generates heatmap to visualize agent's learned actions on the environment\n",
        "    \"\"\"\n",
        "    import seaborn as sns; sns.set()\n",
        "    # display mean of environment values using a heatmap\n",
        "    data = np.mean(q_table, axis = 0)\n",
        "    print(data)\n",
        "    data = data.reshape((5, 12))\n",
        "    ax = sns.heatmap(np.array(data))\n",
        "    return ax\n",
        "    \n",
        "def main():\n",
        "    # Learn state dynamics obtain cumulative rewards for 500 episodes\n",
        "    #SARSA\n",
        "    #q_table_SARSA, reward_cache_SARSA, step_cache_SARSA = sarsa()\n",
        "    # QLEARNING\n",
        "    q_table_qlearning, reward_cache_qlearning, step_cache_qlearning = qlearning()\n",
        "    #plot_number_steps(step_cache_qlearning, step_cache_SARSA)\n",
        "    # Visualize the result\n",
        "    #plot_cumreward_normalized(reward_cache_qlearning,reward_cache_SARSA)\n",
        "    \n",
        "    # generate heatmap\n",
        "    print(\"Visualize environment Q-learning\")\n",
        "    ax_q = generate_heatmap(q_table_qlearning)\n",
        "    print(ax_q)\n",
        "    \n",
        "    #print(\"Visualize SARSA\")\n",
        "   # ax_SARSA = generate_heatmap(q_table_SARSA)\n",
        "    #print(ax_SARSA)\n",
        "    \n",
        "    # Debug method giving information about what are some states for environment\n",
        "    want_to_see_env = False\n",
        "    if(want_to_see_env):\n",
        "        print(\"UP\")\n",
        "        retrieve_environment(q_table_qlearning, 0)\n",
        "        print(\"LEFT\")\n",
        "        retrieve_environment(q_table_qlearning, 1)\n",
        "        print(\"RIGHT\")\n",
        "        retrieve_environment(q_table_qlearning, 2)\n",
        "        print(\"DOWN\")\n",
        "        retrieve_environment(q_table_qlearning, 3)\n",
        "    want_to_see_env = False\n",
        "    if(want_to_see_env):\n",
        "        print(\"UP\")\n",
        "        retrieve_environment(q_table_SARSA, 0)\n",
        "        print(\"LEFT\")\n",
        "        retrieve_environment(q_table_SARSA, 1)\n",
        "        print(\"RIGHT\")\n",
        "        retrieve_environment(q_table_SARSA, 2)\n",
        "        print(\"DOWN\")\n",
        "        retrieve_environment(q_table_SARSA, 3)\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    # call main function to execute grid world\n",
        "    main()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Agent trained with Q-learning after 300 goal reachings\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "cumulative\n",
            "-21.821522309711288\n",
            "Length of episode\n",
            "28.61315789473684\n",
            "TS\n",
            "386\n",
            "success\n",
            "Visualize environment Q-learning\n",
            "[-5.65340943 -5.28483889 -4.7609321  -4.17881344 -3.53201494 -2.81334995\n",
            " -2.01491843 -1.12765401 -0.14244047 -0.37441112  0.94185005 -0.70766162\n",
            " -5.28483889 -4.88802539 -4.32002821 -3.68892023 -2.98768915 -2.2085435\n",
            " -1.34282611 -0.38092652  0.6871144   1.86379216  2.08791377  1.42451702\n",
            " -4.7609321  -4.32002821 -3.68892023 -2.98768915 -2.2085435  -1.34282611\n",
            " -0.38091791  0.687869    1.87541     3.19489816  4.66098768  4.065\n",
            " -4.47693357 -5.43231243 -4.92479159 -4.36087955 -3.73431061 -3.0380979\n",
            " -2.2645785  -1.405065   -0.4251      0.636       2.04        7.6\n",
            " -6.0459872   0.          0.          0.          0.          0.\n",
            "  0.          0.          0.          0.          0.          0.        ]\n",
            "AxesSubplot(0.125,0.125;0.62x0.755)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}